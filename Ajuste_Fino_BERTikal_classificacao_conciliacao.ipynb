{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filipemcbarros/modelos_classificadores_conciliacao/blob/main/Ajuste_Fino_BERTikal_classificacao_conciliacao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keghPgCXT5dr"
      },
      "source": [
        "# **Bibliotecas utilizadas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkaoQj9aT4Tn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,roc_auc_score,roc_curve,accuracy_score,matthews_corrcoef,f1_score\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import os\n",
        "\n",
        "!pip install unidecode\n",
        "!pip install ftfy\n",
        "!pip install transformers==4.2.2\n",
        "!pip install pyreadr\n",
        "!pip install git+https://github.com/felipemaiapolo/legalnlp\n",
        "!pip install transformers torch scikit-learn pandas\n",
        "!pip install simpletransformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importação da biblioteca *simple transformers* para uso pelo Modelo de Linguagem**"
      ],
      "metadata": {
        "id": "Ph1hdrNwJvTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from simpletransformers.language_modeling import (LanguageModelingModel)"
      ],
      "metadata": {
        "id": "tFc5vkzcJjxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpqUOmnpt8w6"
      },
      "source": [
        "## **Método para carregamento do *dataset***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET6hULQ9LM6c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1br3VC0JGmH"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/base_processos/dataset_ajuste_fino.csv\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOOwKC0WPEhO"
      },
      "source": [
        "# **Carregamento e Transformação dos Dados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TI0QIH-PABs"
      },
      "outputs": [],
      "source": [
        "# Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset['corpus_bruto'], dataset['conciliado'], random_state = 1234, test_size = 0.3)\n",
        "\n",
        "doc_train = X_train\n",
        "doc_valid = X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yywKorvjv4sc"
      },
      "outputs": [],
      "source": [
        "path_dados = '' #path com os dados pré-processados\n",
        "path_transformados = '' #path para salvar os dados transformados\n",
        "path_model = '' #path para salvar o modelo com ajuste fino\n",
        "path_save_model = os.path.join(path_model,'model')\n",
        "path_results = path_model + 'Resultados/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4g_70Mx4387"
      },
      "outputs": [],
      "source": [
        "train_file = os.path.join(path_model,'train.txt')\n",
        "valid_file = os.path.join(path_model,'valid.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0_pSwQC4-Mq"
      },
      "outputs": [],
      "source": [
        "with open(train_file,'w') as file:\n",
        "  for doc in doc_train.values:\n",
        "    file.write(doc+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9x91U-m5tJT"
      },
      "outputs": [],
      "source": [
        "with open(valid_file,'w') as file:\n",
        "  for doc in doc_valid.values:\n",
        "    file.write(doc+'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCWPUwqeQUMV"
      },
      "source": [
        "# **Parâmetros do Ajuste Fino do Modelo (*Fine-Tuning*)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ujGqPMkQOEB"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "train_args = {\n",
        "    \"reprocess_input_data\": True,\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"block_size\": 256,\n",
        "    \"max_seq_length\": 256,\n",
        "    \"learning_rate\": 2e-5,\n",
        "    \"train_batch_size\": 8,\n",
        "    'eval_batch_size': 8,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"num_train_epochs\": 3,\n",
        "    \"mlm\": True,\n",
        "    \"mlm_probability\": 0.15,\n",
        "    \"sliding_window\": True,\n",
        "    \"stride\" : 0.8,\n",
        "    'evaluate_during_training': False,\n",
        "    'evaluate_during_training_steps': 140000,\n",
        "    \"output_dir\": os.path.join(path_model,'model'),\n",
        "    \"save_model_every_epoch\": True,\n",
        "    'hidden_dropout_prob': 0.1,\n",
        "    'overwrite_output_dir': True\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmKWZEunRM1E"
      },
      "outputs": [],
      "source": [
        "from legalnlp.get_premodel import *\n",
        "\n",
        "# Fazendo o download do modelo pre-treinado BERTikal e o seu tokenizador\n",
        "get_premodel('bert')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZae6OLwEwGc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as tdata\n",
        "import torch.optim as optim\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QZeMNYuB-Rg"
      },
      "outputs": [],
      "source": [
        "model = LanguageModelingModel(\"bert\", '/content/BERTikal/', args=train_args, use_cuda=True, cuda_device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmB2IYLYRf8N"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "model.train_model(train_file, eval_file=valid_file, show_running_loss=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92nk2VPTbTdV"
      },
      "outputs": [],
      "source": [
        "results_eval = model.eval_model(valid_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqI4cqTkmI5k"
      },
      "source": [
        "# **Carregamento do modelo e do tokenizador treinado com ajuste fino**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTUmXOpPGeKU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as tdata\n",
        "import torch.optim as optim\n",
        "import transformers\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "from transformers import BertForPreTraining, BertModel, BertTokenizer, BertForMaskedLM, BertForNextSentencePrediction, BertForQuestionAnswering\n",
        "import IPython\n",
        "from IPython.display import Image\n",
        "from IPython.display import clear_output\n",
        "\n",
        "bert_model =  BertModel.from_pretrained('/content/model/').to(device)\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('/content/model/checkpoint-24615-epoch-3/vocab.txt', do_lower_case=False)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# configuração do BERT\n",
        "bert_model.config\n",
        "\n",
        "tokenizer = bert_tokenizer\n",
        "model = bert_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVADTtb3HrtB"
      },
      "outputs": [],
      "source": [
        "data_text = list(dataset['corpus_bruto'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J7_DDfv0H3TI"
      },
      "outputs": [],
      "source": [
        "# Definir o DataLoader com seus dados\n",
        "#train_loader = DataLoader(dataset, batch_size=16, shuffle=True) # batch_size(varia de 8-32)\n",
        "\n",
        "\n",
        "# Aplicando o bert_tokenizer em nosso dataset com um comprimento máximo de 512 tokens\n",
        "encoded_inputs = bert_tokenizer(data_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "#Agora temos nossos encoded_input em um dicionário com 3 chaves\n",
        "encoded_inputs.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K8j-c0UeIYF3"
      },
      "outputs": [],
      "source": [
        "# Enviando os tensores para para a GPU\n",
        "input_ids = encoded_inputs['input_ids'].to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouCMaQlvIZ5N"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Criando o nosso vetor de features\n",
        "features = []\n",
        "\n",
        "# Aplicando o modelo pré-treinado em cada frase e adicionando-o ao nosso vetor\n",
        "\n",
        "for i in tqdm(range(len(data_text))):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      last_hidden_states = bert_model(input_ids[i:(i+1)])[1].cpu().numpy().reshape(-1).tolist()\n",
        "\n",
        "    features.append(last_hidden_states)\n",
        "\n",
        "# Criando um numpy array com as features extraidas\n",
        "features = np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYCnKsN8IhBt"
      },
      "outputs": [],
      "source": [
        "df_features = pd.DataFrame(features)\n",
        "features_label = pd.concat([df_features, dataset['conciliado']], axis = 1)\n",
        "features_label.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzH4tyABIjTF"
      },
      "outputs": [],
      "source": [
        "print(features_label)\n",
        "!pip install catboost\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHEaCkOzS1a7"
      },
      "source": [
        "# **Definindo Conjunto de Treinamento e Testes para os modelos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zeky3kuoWrE2"
      },
      "outputs": [],
      "source": [
        "# Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_label.drop(columns = ['conciliado']), features_label['conciliado'], random_state = 1234,test_size = 0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvqWUWVmB6eL"
      },
      "source": [
        "# SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lXe-Q3_JCpdy"
      },
      "outputs": [],
      "source": [
        "# Treinamento do modelo SVC Gaussian Kernel\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svcGaussian = SVC(kernel='rbf', C=1000.0)\n",
        "svcGaussian.fit(X_train,y_train)\n",
        "target_svc_gaussian = svcGaussian.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bXU_WBpTAZf"
      },
      "source": [
        "# **Avaliação do modelo SVC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nlw0a7R2Cxsb"
      },
      "outputs": [],
      "source": [
        "print('############# Relatório de Classificação - Modelo SVC com ajuste fino BERT #############')\n",
        "print()\n",
        "\n",
        "print(\"Relatório de Classificação:\\n\", classification_report(y_test, target_svc_gaussian, digits=4))\n",
        "# imprimir a acurácia do modelo\n",
        "print(\"Acurácia: {:.4f}\\n\".format(accuracy_score(y_test, target_svc_gaussian)))\n",
        "# imprimir a área sob da curva\n",
        "print(\"AUC: {:.4f}\\n\".format(roc_auc_score(y_test, target_svc_gaussian)))\n",
        "print(\"MCC:  {:.4f}\\n\".format(matthews_corrcoef(y_test, target_svc_gaussian)))\n",
        "print(\"F1-Score:  {:.4f}\\n\".format(f1_score(y_test, target_svc_gaussian)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WM89u30mC6M8"
      },
      "outputs": [],
      "source": [
        "print('#### Matriz de Confusão - Modelo SVC com ajuste fino BERT ####')\n",
        "print()\n",
        "# Calculate confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, target_svc_gaussian)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cf_matrix, annot=True, fmt='.0f')\n",
        "plt.title('Matriz de Confusão com ajuste fino BERT')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvLoVRfImgbG"
      },
      "source": [
        "# **Regressão Logística**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yl7TazM7byyO"
      },
      "outputs": [],
      "source": [
        "# Treinamento do modelo Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(solver='lbfgs', C=10.0)\n",
        "lr.fit(X_train,y_train)\n",
        "target_lr = lr.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsjKQnZhIy2S"
      },
      "source": [
        "# **Avaliação do modelo Regressão Logística**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzpDDCGhJuq5"
      },
      "outputs": [],
      "source": [
        "print('############# Relatório de Classificação - Modelo Regressão Logística com ajuste fino BERT #############')\n",
        "print()\n",
        "\n",
        "print(\"Relatório de Classificação:\\n\", classification_report(y_test, target_lr, digits=4))\n",
        "# imprimir a acurácia do modelo\n",
        "print(\"Acurácia: {:.4f}\\n\".format(accuracy_score(y_test, target_lr)))\n",
        "# imprimir a área sob da curva\n",
        "print(\"AUC: {:.4f}\\n\".format(roc_auc_score(y_test, target_lr)))\n",
        "print(\"MCC:  {:.4f}\\n\".format(matthews_corrcoef(y_test, target_lr)))\n",
        "print(\"F1-Score:  {:.4f}\\n\".format(f1_score(y_test, target_lr)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0w2QBwEJ3XV"
      },
      "outputs": [],
      "source": [
        "print('#### Matriz de Confusão - Modelo Regressão Logística com ajuste fino BERT ####')\n",
        "print()\n",
        "# Calculate confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, target_lr)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cf_matrix, annot=True, fmt='.0f')\n",
        "plt.title('Matriz de Confusão com ajuste fino Modelo BERT')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9ceX1Jjm_2i"
      },
      "source": [
        "# Árvore de Decisão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la8fBfP0nECo"
      },
      "outputs": [],
      "source": [
        "# Treinamento do modelo Árvore de Decisão\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dtc = DecisionTreeClassifier(criterion='entropy')\n",
        "dtc.fit(X_train,y_train)\n",
        "target_dtc = dtc.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUVn2WLoI3dA"
      },
      "source": [
        "# **Avaliação do modelo Árvore de Decisão**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNEZtBNbqxME"
      },
      "outputs": [],
      "source": [
        "print('############# Relatório de Classificação - Modelo Árvore de Decisão com ajuste fino BERT #############')\n",
        "print()\n",
        "\n",
        "print(\"Relatório de Classificação:\\n\", classification_report(y_test, target_dtc, digits=4))\n",
        "# imprimir a acurácia do modelo\n",
        "print(\"Acurácia: {:.4f}\\n\".format(accuracy_score(y_test, target_dtc)))\n",
        "# imprimir a área sob da curva\n",
        "print(\"AUC: {:.4f}\\n\".format(roc_auc_score(y_test, target_dtc)))\n",
        "print(\"MCC:  {:.4f}\\n\".format(matthews_corrcoef(y_test, target_dtc)))\n",
        "print(\"F1-Score:  {:.4f}\\n\".format(f1_score(y_test, target_dtc)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "870_CECRq8bD"
      },
      "outputs": [],
      "source": [
        "print('#### Matriz de Confusão - Modelo Árvore de Decisão com ajuste fino BERT ####')\n",
        "print()\n",
        "# Calculate confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, target_dtc)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cf_matrix, annot=True, fmt='.0f')\n",
        "plt.title('Matriz de Confusão com ajuste fino BERT')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVESmay7Bv2a"
      },
      "source": [
        "# XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NNA-0u1Ar-U"
      },
      "outputs": [],
      "source": [
        "# Treinamento do modelo XGBoost\n",
        "import xgboost as xgb\n",
        "\n",
        "# Convert the data into DMatrix format, which is required by XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set the parameters for XGBoost\n",
        "params = {\n",
        "    'criterion':'squared_error',\n",
        "    'max_depth': 7,\n",
        "    'learning_rate': 0.3,\n",
        "    'n_estimators': 100\n",
        "}\n",
        "\n",
        "# Train the Boosted Trees model\n",
        "bst = xgb.train(params, dtrain)\n",
        "\n",
        "# Make predictions on the test set\n",
        "target_xgb = bst.predict(dtest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNZx5hUYI7B5"
      },
      "source": [
        "# **Avaliação do modelo XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfRdkIV3rvrc"
      },
      "outputs": [],
      "source": [
        "# Definir um limite de decisão (exemplo: 0.5)\n",
        "limite_decisao = 0.5\n",
        "\n",
        "# Converter as previsões em classes binárias\n",
        "previsoes_binarias = [1 if predicao > limite_decisao else 0 for predicao in target_xgb]\n",
        "\n",
        "# Converter os rótulos reais em classes binárias\n",
        "rotulos_binarios = [1 if rotulo > limite_decisao else 0 for rotulo in y_test]\n",
        "\n",
        "print('############# Relatório de Classificação - Modelo XGBoost com ajuste fino BERT #############')\n",
        "print()\n",
        "\n",
        "print(\"Relatório de Classificação:\\n\", classification_report(rotulos_binarios, previsoes_binarias, digits=4))\n",
        "# imprimir a acurácia do modelo\n",
        "print(\"Acurácia: {:.4f}\\n\".format(accuracy_score(rotulos_binarios, previsoes_binarias)))\n",
        "# imprimir a área sob da curva\n",
        "print(\"AUC: {:.4f}\\n\".format(roc_auc_score(rotulos_binarios, previsoes_binarias)))\n",
        "print(\"MCC:  {:.4f}\\n\".format(matthews_corrcoef(rotulos_binarios, previsoes_binarias)))\n",
        "print(\"F1-Score:  {:.4f}\\n\".format(f1_score(rotulos_binarios, previsoes_binarias)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwpSDxJSrvyS"
      },
      "outputs": [],
      "source": [
        "print('#### Matriz de Confusão - Modelo XGBoost com ajuste fino BERT ####')\n",
        "print()\n",
        "# Calculate confusion matrix\n",
        "cf_matrix = confusion_matrix(y_test, previsoes_binarias)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cf_matrix, annot=True, fmt='.0f')\n",
        "plt.title('Matriz de Confusão com ajuste fino BERT')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}