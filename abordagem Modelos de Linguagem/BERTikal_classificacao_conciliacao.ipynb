{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filipemcbarros/modelos_classificadores_conciliacao/blob/main/BERTikal_classificacao_conciliacao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keghPgCXT5dr"
      },
      "source": [
        "# **Bibliotecas utilizadas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkaoQj9aT4Tn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,roc_auc_score,roc_curve,accuracy_score,matthews_corrcoef,f1_score\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycnIuNU5vNeq"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode\n",
        "!pip install ftfy\n",
        "!pip install transformers==4.2.2\n",
        "!pip install pyreadr\n",
        "!pip install git+https://github.com/felipemaiapolo/legalnlp\n",
        "!pip install transformers torch scikit-learn pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET6hULQ9LM6c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpqUOmnpt8w6"
      },
      "source": [
        "## **Método para carregamento das localidades**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrLhBpEl4KjT"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/base_processos/dataset_final.csv\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yVTBMSgdf0H"
      },
      "source": [
        "# Funções de Limpeza para Corpus Bruto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOWtpxZQVzP4"
      },
      "outputs": [],
      "source": [
        "# Remover linhas vazias\n",
        "dataset = dataset.dropna(subset=['corpus_limpo'])\n",
        "\n",
        "# Contando o número de palavras na coluna 'corpus_limpo'\n",
        "dataset['corpus_limpo'] = dataset['corpus_limpo'].fillna('')\n",
        "dataset['num_palavras_corpus'] = dataset['corpus_limpo'].str.split().apply(len)\n",
        "\n",
        "# Limpeza das linhas com corpus que possuem menos que 30 palavras para eliminar textos de referências a anexos e sem conteúdo útil\n",
        "dataset = dataset.drop(dataset[dataset['num_palavras_corpus'] < 30].index)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6sCUAwxvANT"
      },
      "outputs": [],
      "source": [
        "from legalnlp.clean_functions import clean_bert\n",
        "\n",
        "dataset['corpus_bruto'] = dataset['corpus_bruto'].apply(lambda x:clean_bert(x))\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKvp9uQ6deoE"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "#texto caixa baixa\n",
        "#remover aspas simples, aspas duplas, reticências, vírgula, underline\n",
        "#remover quebras de linha por espaço em branco simples\n",
        "def clean_text_extra_round(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('[‘’“”\\…,°ºª§]', ' ', text)\n",
        "    text = text.replace(\"'\",\"\")\n",
        "    text = text.replace(\"_\",\"\")\n",
        "    text = text.strip('\\n')\n",
        "    text = text.strip('\\t')\n",
        "    text = clean_roman_numbers(text)\n",
        "    text = re.sub(\"\\s+\", ' ', text)\n",
        "    text = text.replace(\"\\\\\", ' ')\n",
        "    return text\n",
        "\n",
        "#substituir múltiplos espaços em branco por um único espaço em branco\n",
        "def multiple_one_blank_spaces(text):\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text\n",
        "\n",
        "#remove números\n",
        "def is_digit(text):\n",
        "  text = re.sub('\\w*\\d\\w*', '', text)\n",
        "  return text\n",
        "\n",
        "#remove pontuações\n",
        "def is_punctuation(text):\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  return text\n",
        "\n",
        "#remove algarismos romanos\n",
        "def clean_roman_numbers(text):\n",
        "    pattern = r\"\\b(?=[mdclxvii])m{0,4}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})([ii]x|[ii]v|v?[ii]{0,3})\\b\\.?\"\n",
        "    return re.sub(pattern, ' ', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-ZAuouTdr0b"
      },
      "outputs": [],
      "source": [
        "dataset['corpus_bruto'] = dataset['corpus_bruto'].apply(clean_text_extra_round)\n",
        "dataset['corpus_bruto'] = dataset['corpus_bruto'].apply(is_digit)\n",
        "dataset['corpus_bruto'] = dataset['corpus_bruto'].apply(is_punctuation)\n",
        "dataset['corpus_bruto'] = dataset['corpus_bruto'].apply(clean_roman_numbers)\n",
        "dataset['corpus_bruto'] = dataset['corpus_bruto'].apply(multiple_one_blank_spaces)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXpH6tDITJ5w"
      },
      "outputs": [],
      "source": [
        "features = dataset['corpus_bruto']\n",
        "classes = dataset['conciliado']\n",
        "processos = dataset['num_processo']\n",
        "\n",
        "conciliados = features[(dataset['conciliado'] == 1)]\n",
        "nao_conciliados = features[(dataset['conciliado'] == 0)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA1bVKCEEAO4"
      },
      "source": [
        "# **Modelo de Linguagem BERTikal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzCFAdTGgwba"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as tdata\n",
        "import torch.optim as optim\n",
        "import transformers\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "from transformers import BertForPreTraining, BertModel, BertTokenizer, BertForMaskedLM, BertForNextSentencePrediction, BertForQuestionAnswering\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUAvuJUyuCvg"
      },
      "outputs": [],
      "source": [
        "from legalnlp.get_premodel import *\n",
        "# Fazendo o download do modelo pre-treinado BERTikal e o seu tokenizador\n",
        "get_premodel('bert')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SndVpj9uuHG"
      },
      "outputs": [],
      "source": [
        "# Caso esteja usando o Google Colab, não esqueça de ligar a GPU para o ambiente de execução\n",
        "import IPython\n",
        "from IPython.display import Image\n",
        "from IPython.display import clear_output\n",
        "\n",
        "bert_model =  BertModel.from_pretrained('/content/BERTikal/').to(device)\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('/content/BERTikal/vocab.txt', do_lower_case=False)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# configuração do BERTikal\n",
        "bert_model.config\n",
        "\n",
        "tokenizer = bert_tokenizer\n",
        "model = bert_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naMfqk-PI5FP"
      },
      "outputs": [],
      "source": [
        "data_text = list(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I23OuEokfPjd"
      },
      "outputs": [],
      "source": [
        "# Aplicando o bert_tokenizer em nosso dataset com um comprimento máximo de 512 tokens\n",
        "encoded_inputs = bert_tokenizer(data_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "#Agora temos nossos encoded_input em um dicionário com 3 chaves\n",
        "encoded_inputs.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQIn6sHQRBUK"
      },
      "outputs": [],
      "source": [
        "# Enviando os tensores para para a GPU\n",
        "input_ids = encoded_inputs['input_ids'].to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S52WJSPjRVR3"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Criando o nosso vetor de features\n",
        "features = []\n",
        "\n",
        "# Aplicando o modelo pré-treinado em cada frase e adicionando-o ao nosso vetor\n",
        "\n",
        "for i in tqdm(range(len(data_text))):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      last_hidden_states = bert_model(input_ids[i:(i+1)])[1].cpu().numpy().reshape(-1).tolist()\n",
        "\n",
        "    features.append(last_hidden_states)\n",
        "\n",
        "# Criando um numpy array com as features extraidas\n",
        "features = np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7i6ligIRsho"
      },
      "outputs": [],
      "source": [
        "df_features = pd.DataFrame(features)\n",
        "features_label = pd.concat([df_features, dataset['conciliado']], axis = 1)\n",
        "features_label.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(features_label)"
      ],
      "metadata": {
        "id": "e-_xiqWZYFKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B048AFgXppK"
      },
      "outputs": [],
      "source": [
        "!pip install catboost\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHEaCkOzS1a7"
      },
      "source": [
        "# **Definindo Conjunto de Treinamento e Testes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zeky3kuoWrE2"
      },
      "outputs": [],
      "source": [
        "# Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_label.drop(columns = ['conciliado']), features_label['conciliado'], random_state = 1234,test_size = 0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVC"
      ],
      "metadata": {
        "id": "MvqWUWVmB6eL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "#parâmetros com valores decididos pela função GridSearchCV\n",
        "svcGaussian = SVC(kernel='rbf', C=1000.0)\n",
        "svcGaussian.fit(X_train,y_train)\n",
        "target_svc_gaussian = svcGaussian.predict(X_test)"
      ],
      "metadata": {
        "id": "lXe-Q3_JCpdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bXU_WBpTAZf"
      },
      "source": [
        "# **Avaliação do modelo SVC**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('############# Relatório de Classificação - Modelo SVM #############')\n",
        "print()\n",
        "\n",
        "print(\"Relatório de Classificação:\\n\", classification_report(y_test_carregado, target_svc_gaussian, digits=4))\n",
        "# imprimir a acurácia do modelo\n",
        "print(\"Acurácia: {:.4f}\\n\".format(accuracy_score(y_test_carregado, target_svc_gaussian)))\n",
        "# imprimir a área sob da curva\n",
        "print(\"AUC: {:.4f}\\n\".format(roc_auc_score(y_test_carregado, target_svc_gaussian)))\n",
        "print(\"MCC:  {:.4f}\\n\".format(matthews_corrcoef(y_test_carregado, target_svc_gaussian)))\n",
        "print(\"F1-Score:  {:.4f}\\n\".format(f1_score(y_test_carregado, target_svc_gaussian)))"
      ],
      "metadata": {
        "id": "nlw0a7R2Cxsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('#### Matriz de Confusão - Modelo SVM ####')\n",
        "print()\n",
        "cf_matrix = confusion_matrix(y_test_carregado,target_svc_gaussian)\n",
        "sns.heatmap(cf_matrix, annot=True, fmt='.0f')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WM89u30mC6M8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressão Logística"
      ],
      "metadata": {
        "id": "UIuxPaM3nHum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#parâmetros com valores decididos pela função GridSearchCV\n",
        "lr = LogisticRegression(solver='lbfgs', C=10.0)\n",
        "lr.fit(X_train,y_train)\n",
        "target_lr = lr.predict(X_test)"
      ],
      "metadata": {
        "id": "yl7TazM7byyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRZTE_UnpGUP"
      },
      "source": [
        "# **Avaliação do modelo Regressão Logística**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('############# Relatório de Classificação - Modelo Regressão Logística #############')\n",
        "print()\n",
        "\n",
        "print(\"Relatório de Classificação:\\n\", classification_report(y_test_carregado, target_lr, digits=4))\n",
        "# imprimir a acurácia do modelo\n",
        "print(\"Acurácia: {:.4f}\\n\".format(accuracy_score(y_test_carregado, target_lr)))\n",
        "# imprimir a área sob da curva\n",
        "print(\"AUC: {:.4f}\\n\".format(roc_auc_score(y_test_carregado, target_lr)))\n",
        "print(\"MCC:  {:.4f}\\n\".format(matthews_corrcoef(y_test_carregado, target_lr)))\n",
        "print(\"F1-Score:  {:.4f}\\n\".format(f1_score(y_test_carregado, target_lr)))"
      ],
      "metadata": {
        "id": "OnWNe2WZnK6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('#### Matriz de Confusão - Modelo Regressão Logística ####')\n",
        "print()\n",
        "cf_matrix = confusion_matrix(y_test_carregado,target_lr)\n",
        "sns.heatmap(cf_matrix, annot=True, fmt='.0f')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gsIzyhsnnN70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Árvore de Decisão"
      ],
      "metadata": {
        "id": "RyceMW3CpY4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#parâmetros com valores decididos pela função GridSearchCV\n",
        "dtc = DecisionTreeClassifier(criterion='“entropy”')\n",
        "dtc.fit(word2vec_train_vectors, y_train)\n",
        "target_dtc = dtc.predict(word2vec_test_vectors)"
      ],
      "metadata": {
        "id": "WOD5cv4-pY4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOrbE4eRpY4x"
      },
      "source": [
        "# **Avaliação do modelo Árvore de Decisão**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('############# Relatório de Classificação - Modelo Árvore de Decisão #############')\n",
        "print()\n",
        "\n",
        "print(\"Relatório de Classificação:\\n\", classification_report(y_test_carregado, target_dtc, digits=4))\n",
        "# imprimir a acurácia do modelo\n",
        "print(\"Acurácia: {:.4f}\\n\".format(accuracy_score(y_test_carregado, target_dtc)))\n",
        "# imprimir a área sob da curva\n",
        "print(\"AUC: {:.4f}\\n\".format(roc_auc_score(y_test_carregado, target_dtc)))\n",
        "print(\"MCC:  {:.4f}\\n\".format(matthews_corrcoef(y_test_carregado, target_dtc)))\n",
        "print(\"F1-Score:  {:.4f}\\n\".format(f1_score(y_test_carregado, target_dtc)))"
      ],
      "metadata": {
        "id": "ZSNSfEsDpY4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('#### Matriz de Confusão - Modelo Árvore de Decisão ####')\n",
        "print()\n",
        "cf_matrix = confusion_matrix(y_test_carregado,target_dtc)\n",
        "sns.heatmap(cf_matrix, annot=True, fmt='.0f')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J_W0uM_CpY4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost"
      ],
      "metadata": {
        "id": "bVESmay7Bv2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento do modelo XGBoost\n",
        "import xgboost as xgb\n",
        "\n",
        "# Convert the data into DMatrix format, which is required by XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set the parameters for XGBoost\n",
        "params = {\n",
        "    'criterion':'squared_error',\n",
        "    'max_depth': 7,\n",
        "    'learning_rate': 0.3,\n",
        "    'n_estimators': 100\n",
        "}\n",
        "\n",
        "# Train the Boosted Trees model\n",
        "bst = xgb.train(params, dtrain)\n",
        "\n",
        "# Make predictions on the test set\n",
        "target_xgb = bst.predict(dtest)"
      ],
      "metadata": {
        "id": "5NNA-0u1Ar-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BrSGnJ0pLYO"
      },
      "source": [
        "# **Avaliação do modelo XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir um limite de decisão (exemplo: 0.5)\n",
        "limite_decisao = 0.5\n",
        "\n",
        "# Converter as previsões em classes binárias\n",
        "previsoes_binarias = [1 if predicao > limite_decisao else 0 for predicao in target_xgb]\n",
        "\n",
        "# Converter os rótulos reais em classes binárias\n",
        "rotulos_binarios = [1 if rotulo > limite_decisao else 0 for rotulo in y_test]\n",
        "\n",
        "print('############# Relatório de Classificação - Modelo XGBoost #############')\n",
        "print()\n",
        "\n",
        "print(\"Relatório de Classificação:\\n\", classification_report(rotulos_binarios, previsoes_binarias, digits=4))\n",
        "\n",
        "print(\"Acurácia: {:.4f}\\n\".format(accuracy_score(rotulos_binarios, previsoes_binarias)))\n",
        "# imprimir a área sob da curva\n",
        "print(\"AUC: {:.4f}\\n\".format(roc_auc_score(rotulos_binarios, previsoes_binarias)))\n",
        "print(\"MCC:  {:.4f}\\n\".format(matthews_corrcoef(rotulos_binarios, previsoes_binarias)))\n",
        "print(\"F1-Score:  {:.4f}\\n\".format(f1_score(rotulos_binarios, previsoes_binarias)))"
      ],
      "metadata": {
        "id": "8JCMfjOvlZRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('#### Matriz de Confusão - Modelo XGBoost ####')\n",
        "print()\n",
        "cf_matrix = confusion_matrix(y_test,previsoes_binarias)\n",
        "sns.heatmap(cf_matrix, annot=True, fmt='.0f')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "daDii_Onlbv_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
